! Lagrange_Cuda_Class.cuf
! 
! Copyright 2017 Joseph Schoonover <joe@fluidnumerics.consulting>, Fluid Numerics LLC
! All rights reserved.
!
! //////////////////////////////////////////////////////////////////////////////////////////////// !


MODULE Lagrange_Cuda_Class

!src/common
USE ModelPrecision
USE ConstantsDictionary
!USE CommonRoutines
! src/interp
USE Lagrange_Class

! CUDA libraries
USE cudafor

IMPLICIT NONE



   TYPE, EXTENDS( Lagrange ) :: Lagrange_Cuda
      REAL(prec), ALLOCATABLE, DEVICE :: bWs_dev(:)  ! barycentric weights
      REAL(prec), ALLOCATABLE, DEVICE :: T_dev(:,:) ! Interpolation matrix to get us from what we have to what
                                                     ! we want, in the "s" direction.
      REAL(prec), ALLOCATABLE, DEVICE :: D_dev(:,:) ! Derivative matrix to calculate the derivative of an 
                                         ! interpolant at the interpolation nodes ("s"). 
                                         ! This derivative matrix is used to calculate the 
                                         ! derivative in the "s" computational direction.
      CONTAINS
      
      !-------------!
      ! Constructors/Destructors
      PROCEDURE :: Build => Build_Lagrange_Cuda
      PROCEDURE :: Trash => Trash_Lagrange_Cuda

      PROCEDURE :: ApplyInterpolationMatrices_3D_CUDA => ApplyInterpolationMatrices_3D_Lagrange_Cuda
      PROCEDURE :: ApplyDerivativeMatrices_3D_ACC     => ApplyDerivativeMatrices_3D_Lagrange_OpenACC
      PROCEDURE :: ApplyDerivativeMatrices_3D_CUDA    => ApplyDerivativeMatrices_3D_Lagrange_Cuda
      PROCEDURE :: CalculateDivergence_3D_CUDA        => CalculateDivergence_3D_Lagrange_Cuda
      
      
    END TYPE Lagrange_Cuda

    INTEGER, CONSTANT :: N_dev
    INTEGER, CONSTANT :: M_dev
    
 CONTAINS
!
!
!==================================================================================================!
!------------------------------- Manual Constructors/Destructors ----------------------------------!
!==================================================================================================!
!
!> \addtogroup Lagrange_Cuda_Class 
!! @{ 
! ================================================================================================ !
! S/R Build
!
!>  \fn Build_Lagrange_Cuda
!!  A manual constructor for the Lagrange_Cuda class that allocates memory and fills in data 
!!  for the attributes of the Lagrange_Cuda class.
!! 
!!  The Build subroutine allocates memory for the native and non-native grid points, barycentric
!!  weights interpolation matrix, and derivative matrix. The native and non-native grid points are
!!  filled in using the REAL(prec) input arrays "s" and "so". The barycentric weights are then 
!!  calculated and stored. Once the barycentric weights are calculated, the interpolation and
!!  derivative matrices are calculated and stored.
!!
!!  <H2> Usage : </H2>
!!     <B>TYPE</B>(Lagrange_Cuda) :: this <BR>
!!         .... <BR>
!!     <B>CALL</B> this % Build( N, M, s, so ) <BR>
!!
!!  <table>
!!       <tr> <td> in/out <th> myPoly <td> TYPE(Lagrange_Cuda) <td> The Lagrange_Cuda data structure to 
!!                                                                be constructed
!!       <tr> <td> in <th> N <td> INTEGER <td> The number of native grid points
!!       <tr> <td> in <th> M <td> INTEGER <td> The number of target grid points
!!       <tr> <td> in <th> s(0:N) <td> REAL(prec) <td> The native grid points
!!       <tr> <td> in <th> so(0:N) <td> REAL(prec) <td> The target grid points
!!  </table>
!!
! =============================================================================================== !
!>@}
 ATTRIBUTES(Host) SUBROUTINE Build_Lagrange_Cuda( myPoly, N, M, s, so )

   IMPLICIT NONE
   CLASS(Lagrange_Cuda), INTENT(out)   :: myPoly
   INTEGER, INTENT(in)                 :: N, M
   REAL(prec), INTENT(in)              :: s(0:N), so(0:M)
   
   
      IF( N > 7 )THEN
         PRINT*, 'Module Lagrange_Cuda_Class.cuf : S/R Build_Lagrange_Cuda :'
         PRINT*, 'polyDeg > 7 Not currently supported with CUDA'
         PRINT*, 'STOPPING'
         STOP
      ENDIF
      
   !   IF( M > 7 )THEN
   !      PRINT*, 'Module Lagrange_Cuda_Class.cuf : S/R Build_Lagrange_Cuda :'
   !      PRINT*, 'nPlot > 7 Not currently supported with CUDA'
   !      PRINT*, 'STOPPING'
   !      STOP
   !   ENDIF
         
      ! Set the number of observations (those we have and those we want)
      myPoly % N  = N
      myPoly % Nc = N*(N+2)
      myPoly % M  = M
      myPoly % Mc = M*(M+2)
      
      ! Allocate storage
      ALLOCATE( myPoly % s(0:N), &
                myPoly % bWs(0:N), &
                myPoly % so(0:M), &
                myPoly % Ts(0:M,0:N), &
                myPoly % Tp(0:N,0:M), &
                myPoly % D(0:N,0:N), &
                myPoly % DTr(0:N,0:N) )
      myPoly % s   = 0.0_prec
      myPoly % bWs = 0.0_prec
      myPoly % so  = 0.0_prec
      myPoly % Ts  = 0.0_prec
      myPoly % Tp  = 0.0_prec
      myPoly % D   = 0.0_prec
      myPoly % DTr = 0.0_prec
      
      ! Fill in the nodal locations
      myPoly % s(0:N)  = s(0:N)
      myPoly % so(0:M) = so(0:M)

      ! and calculate the barycentric weights for quick interpolation.
      CALL myPoly % CalculateBarycentricWeights( )

      ! Using the two nodal locations, we can construct the interpolation matrix. The interpolation
      ! matrix enables quick interpolation.
      CALL myPoly % CalculateInterpolationMatrix( )

      CALL myPoly % CalculateDerivativeMatrix( )
      
      ! Allocate device attributes
      ALLOCATE( myPoly % bWs_dev(0:N), &
                myPoly % T_dev(0:N,0:M), &
                myPoly % D_dev(0:N,0:N) )

      ! Copy the data from the host attributes
      myPoly % bWs_dev = myPoly % bWs
      myPoly % T_dev   = myPoly % Tp
      myPoly % D_dev   = myPoly % DTr
      
      N_dev = N
      M_dev = M
      
 END SUBROUTINE Build_Lagrange_Cuda
!
!
!> \addtogroup Lagrange_Cuda_Class 
!! @{ 
! ================================================================================================ !
! S/R Trash 
! 
!> \fn Trash_Lagrange_Cuda
!! A manual destructor for the Lagrange_Cuda class that deallocates the memory held by its 
!!  attributes. 
!! 
!! <H2> Usage : </H2> 
!! <B>TYPE</B>(Lagrange_Cuda) :: this <BR>
!!         .... <BR>
!!     <B>CALL</B> this % Trash( ) <BR>
!!  
!!  <H2> Parameters : </H2>
!!  <table> 
!!   <tr> <td> in/out <th> myPoly <td> TYPE(Lagrange_Cuda) <td> 
!!                       On <B>input</B>, the Lagrange_Cuda data structure with attributes filled in. <BR>
!!                       On <B>output</B>,the memory associated with this data-structure is freed.
!!  </table>  
!!   
! ================================================================================================ ! 
!>@}
 ATTRIBUTES(Host) SUBROUTINE Trash_Lagrange_Cuda(myPoly)

   IMPLICIT NONE
   CLASS(Lagrange_Cuda), INTENT(inout) :: myPoly

      PRINT*, 'Module : TestLagrange_CudaClass_CUDA.cuf : S/R Trash_Lagrange_Cuda :'
      PRINT*, '         Deallocating attributes of the interpolant data structure'
      DEALLOCATE( myPoly % s, myPoly % bWs, myPoly % so, myPoly % Ts, myPoly % D )
      DEALLOCATE( myPoly % bWs_dev, myPoly % T_dev, myPoly % D_dev )
      
 END SUBROUTINE Trash_Lagrange_Cuda
!!
!!
!!==================================================================================================!
!!--------------------------------- Type Specific Routines -----------------------------------------!
!!==================================================================================================!
!!
!!
!
 ATTRIBUTES(Host) FUNCTION ApplyInterpolationMatrices_3D_Lagrange_Cuda( myPoly, f, nElems ) RESULT( fNew )  

   IMPLICIT NONE
   CLASS(Lagrange_Cuda) :: myPoly
   INTEGER, MANAGED     :: nElems
   REAL(prec), DEVICE   :: f(0:myPoly % N,0:myPoly % N,0:myPoly % N,1:nElems)
   REAL(prec), DEVICE   :: fNew(0:myPoly % M,0:myPoly % M,0:myPoly % M,1:nElems)
   ! Local
   TYPE(dim3) :: grid, tBlock
   INTEGER :: threadCount, nTiles
   
      threadCount = MIN( 4*(ceiling( REAL(myPoly % M+1)/4 ) ), 8 )
      nTiles = ceiling( REAL(myPoly % M+1)/threadCount )
      tBlock = dim3( threadCount, threadCount, threadCount )
      grid = dim3(nElems,nTiles,1)  ! Because nElems can reach a rather large size, it must remain the first dimension of the grid (see pgaccelinfo for MAximum Grid Size)
     
      CALL ApplyInterpolationMatrices_3D_CUDAKernel<<<grid, tBlock>>>( myPoly % T_dev, &
                                                                       f, fNew, &
                                                                       nElems )

 END FUNCTION ApplyInterpolationMatrices_3D_Lagrange_Cuda
!
 ATTRIBUTES(Global) SUBROUTINE ApplyInterpolationMatrices_3D_CUDAKernel( IntMatT, f, fnew, nElems  )

   IMPLICIT NONE
   INTEGER, INTENT(in)             :: nElems
   REAL(prec), DEVICE, INTENT(in)  :: IntMatT(0:N_dev,0:N_dev)
   REAL(prec), DEVICE, INTENT(in)  :: f(0:N_dev,0:N_dev,0:N_dev,1:nElems)
   REAL(prec), DEVICE, INTENT(out) :: fnew(0:N_dev,0:N_dev,0:N_dev,1:nElems)
   ! Local
   INTEGER :: i, j, k, m, n, p, iEl
   REAL(prec), SHARED :: floc(0:7,0:7,0:7)
   REAL(prec) :: fm, fmn, fmnp
   
   
      iEl = blockIdx % x
      ! y-dimension of the block indicates which tile we are working with
      ! The block dimension indicates how many threads there are in each block
      m = threadIdx % x-1 + (blockIDx % y-1)*blockDim % x
      n = threadIdx % y-1 + (blockIDx % y-1)*blockDim % y
      p = threadIdx % z-1 + (blockIDx % y-1)*blockDim % z
      
      
      ! Pre-fetch data
      IF( m <= M_dev .AND. n <= M_dev .AND. p <= M_dev )THEN
         floc(m,n,p) = f(m,n,p,iEl)
         
         CALL syncthreads( )
      
               
         fmnp = 0.0_prec
         DO k = 0, N_dev
         
            fmn = 0.0_prec
            DO j = 0, N_dev
            
               fm = 0.0_prec
               DO i = 0, N_dev
                  fm = fm + floc(i,j,k)*IntMatT(i,m)
               ENDDO
               
               fmn = fmn + fm*IntMatT(j,n)
            ENDDO
            
            fmnp = fmnp + fmn*IntMatT(k,p)
         ENDDO
         
         fnew(m,n,p,iEl) = fmnp
         
      ENDIF

 END SUBROUTINE ApplyInterpolationMatrices_3D_CUDAKernel
 
 FUNCTION ApplyDerivativeMatrices_3D_Lagrange_OpenACC( myPoly, f, nElems ) RESULT( derF )  

  IMPLICIT NONE
  CLASS(Lagrange_Cuda) :: myPoly
  INTEGER              :: nElems
  REAL(prec), DEVICE           :: f(0:myPoly % N,0:myPoly % N,0:myPoly % N, 1:nElems)
  REAL(prec), DEVICE          :: derF(1:3,0:myPoly % N,0:myPoly % N,0:myPoly % N, 1:nElems)
  ! Local
  INTEGER :: ii, i, j, k, iEl
  REAL(prec) :: ds, dp, dq
  
      !$acc parallel loop gang vector collapse(4) present( f, derf )
      DO iEl = 1, nElems  ! << Largest Loop O( 100,000 )
         DO k = 0, myPoly % N ! << O( 10 )
            DO j = 0, myPoly % N ! << O( 10 )
               DO i = 0, myPoly % N ! << O( 10 )
               
                  ds = 0.0_prec
                  dp = 0.0_prec
                  dq = 0.0_prec
                  DO ii = 0, myPoly % N ! << O( 10 ), Reduction loop ( vector-vector operation 
                                        ! for a single i,j,k )
                     ds = ds + myPoly % DTr(ii,i)*f(ii,j,k,iEl)
                     dp = dp + myPoly % DTr(ii,j)*f(i,ii,k,iEl)
                     dq = dq + myPoly % DTr(ii,k)*f(i,j,ii,iEl)
                  ENDDO
                  
                  derf(1,i,j,k,iEl) = ds
                  derf(2,i,j,k,iEl) = dp
                  derf(3,i,j,k,iEl) = dq
                  
               ENDDO
            ENDDO
         ENDDO
      ENDDO
     

 END FUNCTION ApplyDerivativeMatrices_3D_Lagrange_OpenACC

! In spectral element methods, it is often the case that the same derivative matrix
! is applied to many "elements" of data. Each element contains a structured 3-D block of 
! data that the derivative matrix can be applied to
 ATTRIBUTES(Host) FUNCTION ApplyDerivativeMatrices_3D_Lagrange_Cuda( myPoly, f, nElems ) RESULT( derF )  

  IMPLICIT NONE
  CLASS(Lagrange_Cuda) :: myPoly
  INTEGER, MANAGED     :: nElems
  REAL(prec), DEVICE   :: f(0:myPoly % N,0:myPoly % N,0:myPoly % N, 1:nElems)
  REAL(prec), DEVICE   :: derF(1:3,0:myPoly % N,0:myPoly % N,0:myPoly % N, 1:nElems)
  ! Local
  TYPE(dim3) :: grid, tBlock
  
     ! How should we pick the thread and block size
     ! L1-Cache is 64 KB. The function in each element occupies 
     ! precision*(N+1)^3 Bytes. 
     ! Order 7 polynomial with double precision gives 4 KB.
     ! The gradient of the function occupies precision*3*(N+1)^3 Bytes.
     ! Then, the function and its gradient for a single element occupies precision*4*(N+1)^3 Bytes
     ! For an order 7 polynomial with double precision, this is 16 KB
     ! For the GeForce GTX 1060 , there can be at most 2048 threads per Serial multiprocessor
     ! and there are ten multiprocessors
     ! With a little bit of overhead from the kernel stack, we can fit 3 elements simultaneously
     ! all within L1-Cache (48 KB)
     tBlock = dim3(4*(ceiling( REAL(myPoly % N+1)/4 ) ), &
                   4*(ceiling( REAL(myPoly % N+1)/4 ) ) , &
                   4*(ceiling( REAL(myPoly % N+1)/4 ) ) )
     grid = dim3(nElems,1,1)  ! Because nElems can reach a rather large size, it must remain the first dimension of the grid (see pgaccelinfo for MAximum Grid Size)
     
     CALL ApplyDerivativeMatrices_3D_CUDAKernel<<<grid, tBlock>>>( myPoly % D_dev, &
                                                                 f, derF, &
                                                                 nElems )
     

 END FUNCTION ApplyDerivativeMatrices_3D_Lagrange_Cuda
!
 ATTRIBUTES(Global) SUBROUTINE ApplyDerivativeMatrices_3D_CUDAKernel( DMatT, f, df, nElems )
  
  IMPLICIT NONE
  INTEGER, INTENT(in)    :: nElems
  REAL(prec), DEVICE, INTENT(in)  :: DMatT(0:N_dev,0:N_dev)
  REAL(prec), DEVICE, INTENT(in)  :: f(0:N_dev,0:N_dev,0:N_dev,1:nElems)
  REAL(prec), DEVICE, INTENT(out) :: df(1:3,0:N_dev,0:N_dev,0:N_dev,1:nElems)
  ! Local
  INTEGER :: i, j, k, iEl, ii
  REAL(prec), SHARED :: floc(0:7,0:7,0:7)
  REAL(prec) :: dfs, dfp, dfq
  
  
      iEl = blockIdx % x
      i = threadIdx % x-1
      j = threadIdx % y-1
      k = threadIdx % z-1
      
      IF( i <= N_dev .AND. j <= N_dev .AND. k <= N_dev )THEN
      
         
         ! Pre-fetch into shared memory for this block
         floc(i,j,k) = f(i,j,k,iEl)
         CALL syncthreads( )

         dfs = 0.0_prec
         dfp = 0.0_prec
         dfq = 0.0_prec
         DO ii = 0, N_dev
            dfs = dfs + DMatT(ii,i)*floc(ii,j,k)
            dfp = dfp + DMatT(ii,j)*floc(i,ii,k)
            dfq = dfq + DMatT(ii,k)*floc(i,j,ii)
         ENDDO
                  
         df(1,i,j,k,iEl) = dfs
         df(2,i,j,k,iEl) = dfp
         df(3,i,j,k,iEl) = dfq

      ENDIF
    
 END SUBROUTINE ApplyDerivativeMatrices_3D_CUDAKernel
!
 ATTRIBUTES(Host) FUNCTION CalculateDivergence_3D_Lagrange_Cuda( myPoly, f, nElems ) RESULT( divF )  

  IMPLICIT NONE
  CLASS(Lagrange_Cuda) :: myPoly
  INTEGER, MANAGED     :: nElems
  REAL(prec), DEVICE   :: f(1:3,0:myPoly % N,0:myPoly % N,0:myPoly % N, 1:nElems)
  REAL(prec), DEVICE   :: divF(0:myPoly % N,0:myPoly % N,0:myPoly % N, 1:nElems)
  ! Local
  TYPE(dim3) :: grid, tBlock
  
     ! How should we pick the thread and block size
     ! L1-Cache is 64 KB. The function in each element occupies 
     ! precision*(N+1)^3 Bytes. 
     ! Order 7 polynomial with double precision gives 4 KB.
     ! The gradient of the function occupies precision*3*(N+1)^3 Bytes.
     ! Then, the function and its gradient for a single element occupies precision*4*(N+1)^3 Bytes
     ! For an order 7 polynomial with double precision, this is 16 KB
     ! For the GeForce GTX 1060 , there can be at most 2048 threads per Serial multiprocessor
     ! and there are ten multiprocessors
     ! With a little bit of overhead from the kernel stack, we can fit 3 elements simultaneously
     ! all within L1-Cache (48 KB)
     tBlock = dim3(4*(ceiling( REAL(myPoly % N+1)/4 ) ), &
                   4*(ceiling( REAL(myPoly % N+1)/4 ) ) , &
                   4*(ceiling( REAL(myPoly % N+1)/4 ) ) )
     grid = dim3(nElems,1,1)  ! Because nElems can reach a rather large size, it must remain the first dimension of the grid (see pgaccelinfo for MAximum Grid Size)
     
     CALL CalculateDivergence_3D_CUDAKernel<<<grid, tBlock>>>( myPoly % D_dev, &
                                                               f, divF, &
                                                               nElems )
     

 END FUNCTION CalculateDivergence_3D_Lagrange_Cuda
!
 ATTRIBUTES(Global) SUBROUTINE CalculateDivergence_3D_CUDAKernel( DMatT, f, df, nElems )
  
  IMPLICIT NONE
  INTEGER, INTENT(in)    :: nElems
  REAL(prec), DEVICE, INTENT(in)  :: DMatT(0:N_dev,0:N_dev)
  REAL(prec), DEVICE, INTENT(in)  :: f(1:3,0:N_dev,0:N_dev,0:N_dev,1:nElems)
  REAL(prec), DEVICE, INTENT(out) :: df(0:N_dev,0:N_dev,0:N_dev,1:nElems)
  ! Local
  INTEGER :: i, j, k, iEl, ii
  REAL(prec), SHARED :: floc(1:3,0:7,0:7,0:7)
  REAL(prec) :: divf
  
  
      iEl = blockIdx % x
      i = threadIdx % x-1
      j = threadIdx % y-1
      k = threadIdx % z-1
      
      IF( i <= N_dev .AND. j <= N_dev .AND. k <= N_dev )THEN
      
         
         ! Pre-fetch into shared memory for this block
         floc(1,i,j,k) = f(1,i,j,k,iEl)
         floc(2,i,j,k) = f(2,i,j,k,iEl)
         floc(3,i,j,k) = f(3,i,j,k,iEl)
         CALL syncthreads( )

         divf = 0.0_prec
         DO ii = 0, N_dev
            divf = divf + DMatT(ii,i)*floc(1,ii,j,k) + &
                          DMatT(ii,j)*floc(2,i,ii,k) + &
                          DMatT(ii,k)*floc(3,i,j,ii)
         ENDDO
                  
         df(i,j,k,iEl) = divf

      ENDIF
    
 END SUBROUTINE CalculateDivergence_3D_CUDAKernel
 
END MODULE Lagrange_Cuda_Class
